# Block-Wise-Pruning
|Title |Organization|Year|Publisher|
| ----------- | ----------- | ----------- | ----------- |
|[Block Sparsity and Weight Initialization in Neural Network Pruning](https://dspace.mit.edu/handle/1721.1/130708)|MIT|2021|MIT|
|[Block-wise Dynamic Sparseness](https://arxiv.org/pdf/2001.04686.pdf)|Ghent University|2020|Pattern Recognition Letters|
|[Learning Instance-wise Sparsity for Accelerating Deep Models](https://www.ijcai.org/proceedings/2019/0416.pdf)|University of Sydney,Huawei Noahâ€™s Ark Lab|2019|IJCAI-19|
|[Balanced Sparsity for Efficient DNN Inference on GPU](https://arxiv.org/abs/1811.00206)|1Tsinghua University 2Harbin Institute of Technology 3Beihang University 4Microsoft Research Asia|2018|AAAI 2019|
|[Parallel Blockwise Knowledge Distillation for Deep Neural Network Compression](https://arxiv.org/pdf/2012.03096.pdf)| |2021|IEEE Transactions on Parallel and Distributed Systems|
|[SBNet: Sparse Blocks Network for Fast Inference](https://openaccess.thecvf.com/content_cvpr_2018/papers/Ren_SBNet_Sparse_Blocks_CVPR_2018_paper.pdf)|Uber|2018|CVPR|
|[Dynamic Block Sparse Reparameterization of Convolutional Neural Networks](https://openaccess.thecvf.com/content_ICCVW_2019/papers/CEFRL/Vooturi_Dynamic_Block_Sparse_Reparameterization_of_Convolutional_Neural_Networks_ICCVW_2019_paper.pdf)|Center for Security Theory and Algorithmic Research|2019|ICCV|
