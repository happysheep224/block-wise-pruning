# Block-Wise-Pruning
|Title |Organization|Year|Publisher|
| ----------- | ----------- | ----------- | ----------- |
|[Block Sparsity and Weight Initialization in Neural Network Pruning](https://dspace.mit.edu/handle/1721.1/130708)|MIT|2021|MIT|
|[Block-wise Dynamic Sparseness](https://arxiv.org/pdf/2001.04686.pdf)|Ghent University|2020|Pattern Recognition Letters|
|[Learning Instance-wise Sparsity for Accelerating Deep Models](https://www.ijcai.org/proceedings/2019/0416.pdf)|University of Sydney,Huawei Noahâ€™s Ark Lab|2019|IJCAI-19|
|[Balanced Sparsity for Efficient DNN Inference on GPU](https://arxiv.org/abs/1811.00206)|1Tsinghua University 2Harbin Institute of Technology 3Beihang University 4Microsoft Research Asia|2018|AAAI 2019|
|[Parallel Blockwise Knowledge Distillation for Deep Neural Network Compression](https://arxiv.org/pdf/2012.03096.pdf)| |2021|IEEE Transactions on Parallel and Distributed Systems|
