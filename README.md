# Block-Wise-Pruning
|Num|Title |Organization|Year|Publisher|
| ----------- | ----------- | ----------- | ----------- | ----------- |
|1|[Block Sparsity and Weight Initialization in Neural Network Pruning](https://dspace.mit.edu/handle/1721.1/130708)|MIT|2021|MIT|
|2|[Block-wise Dynamic Sparseness](https://arxiv.org/pdf/2001.04686.pdf)|Ghent University|2020|Pattern Recognition Letters|
|3|[Learning Instance-wise Sparsity for Accelerating Deep Models](https://www.ijcai.org/proceedings/2019/0416.pdf)|University of Sydney,Huawei Noahâ€™s Ark Lab|2019|IJCAI-19|
|4|[Balanced Sparsity for Efficient DNN Inference on GPU](https://arxiv.org/abs/1811.00206)|1Tsinghua University 2Harbin Institute of Technology 3Beihang University 4Microsoft Research Asia|2018|AAAI 2019|
|5|[Parallel Blockwise Knowledge Distillation for Deep Neural Network Compression](https://arxiv.org/pdf/2012.03096.pdf)|Texas State University |2021|IEEE Transactions on Parallel and Distributed Systems|
|6|[SBNet: Sparse Blocks Network for Fast Inference](https://openaccess.thecvf.com/content_cvpr_2018/papers/Ren_SBNet_Sparse_Blocks_CVPR_2018_paper.pdf)|Uber|2018|CVPR|
|7|[Dynamic Block Sparse Reparameterization of Convolutional Neural Networks](https://openaccess.thecvf.com/content_ICCVW_2019/papers/CEFRL/Vooturi_Dynamic_Block_Sparse_Reparameterization_of_Convolutional_Neural_Networks_ICCVW_2019_paper.pdf)|Center for Security Theory and Algorithmic Research|2019|ICCV|
|8|[Accelerating Matrix Multiplication with Block Sparse Format and NVIDIA Tensor Cores](https://developer.nvidia.com/blog/accelerating-matrix-multiplication-with-block-sparse-format-and-nvidia-tensor-cores/)|Nivida|2021|Nivadia|
|9|[Block-wise weighted sparse representation-based classification](https://link.springer.com/article/10.1007/s11760-020-01700-9)| |2020|Signal, Image and Video Processing| 
|10|[Dynamic Convolutions: Exploiting Spatial Sparsity for Faster Inference](https://lirias.kuleuven.be/bitstream/123456789/670120/2/4583_final_OA.pdf)|KU Leuven|2020|CVPR
|11|[Spatially Adaptive Inference with Stochastic Feature Sampling and Interpolation](https://arxiv.org/pdf/2003.08866.pdf)|Tsinghua University|2020|ECCV|
# Ref
|Num|Title |Organization|Year|Publisher|
| ----------- | ----------- | ----------- | ----------- | ----------- |
|1|[Dynamic Capacity Networks](https://arxiv.org/pdf/1511.07838.pdf)|Twitter, Cambridge, MA, USA|2016|ICML|
|2|[Conditional computation in neural networks for faster models](https://arxiv.org/pdf/1704.01344.pdf)|The Chinese University of Hong Kong|2015|arXiv|
|3|[Spatially adaptive computation time for residual networks](https://arxiv.org/pdf/1612.02297.pdf)|2Google Inc|2017|cvpr|
|4|[Deep networks with stochastic depth](https://arxiv.org/pdf/1603.09382.pdf)| Cornell University|2016|ECCV|
|5|[Fast algorithms for convolutional neural networks](https://arxiv.org/pdf/1509.09308.pdf)|Nervana Systems|2016|CVPR|
|6|[categorical reparameterization with gumbel-softmax](https://arxiv.org/pdf/1611.01144.pdf)|Google Brain|2017|ICLR|
|7|[Estimating or propagating gradients through stochastic neurons for conditional computation]()||2013|arXiv|
|8|[Spatially-sparse convolutional neural networks](https://arxiv.org/pdf/1409.6070.pdf)|University of Warwick|2014|arXiv|
|9|[Sub-manifold sparse convolutional networks](https://arxiv.org/pdf/1706.01307.pdf)||2017|arXiv|
|10|[Not all pixels are equal: Difficulty-aware semantic segmentation bia deep layer cascade](https://arxiv.org/pdf/1704.01344.pdf)||||
|11|[3D Semantic Segmentation with Submanifold Sparse Convolutional Networks](https://arxiv.org/pdf/1711.10275.pdf)||||
|12|[Not All Pixels Are Equal: Difficulty-Aware Semantic Segmentation via Deep Layer Cascade](https://arxiv.org/pdf/1704.01344.pdf)|

